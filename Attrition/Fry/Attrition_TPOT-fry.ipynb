{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#一 数据收集 start \n",
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No     988\n",
      "Yes    188\n",
      "Name: Attrition, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#二 数据预处理 start \n",
    "print(train['Attrition'].value_counts())\n",
    "train['Attrition']=train['Attrition'].map(lambda x:1 if x=='Yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isna().sum()#查看是否有空值 Step1，对数据进行探索 \n",
    "#Step2，去掉无用特征，处理分类特征\n",
    "train= train.drop(['EmployeeNumber', 'StandardHours'], axis=1)\n",
    "test=test.drop(['EmployeeNumber', 'StandardHours'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分类特征进行特征值编码：\n",
    "from sklearn import preprocessing \n",
    "\n",
    "attr=['Age','BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18','OverTime']\n",
    "lbe_list=[]\n",
    "for fea in attr:\n",
    "    lbe=preprocessing.LabelEncoder()\n",
    "    train[fea]=lbe.fit_transform(train[fea])\n",
    "    test[fea]=lbe.transform(test[fea])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3d1fcceb0648bba9d4531f1625c991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Optimization Progress', max=120.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: 0.8723404255319149\n",
      "Generation 2 - Current best internal CV score: 0.8723404255319149\n",
      "Generation 3 - Current best internal CV score: 0.8723404255319149\n",
      "Generation 4 - Current best internal CV score: 0.8723404255319149\n",
      "Generation 5 - Current best internal CV score: 0.874468085106383\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(StandardScaler(input_matrix), learning_rate=0.1, max_depth=10, max_features=0.6000000000000001, min_samples_leaf=15, min_samples_split=4, n_estimators=100, subsample=0.2)\n",
      "0.8177966101694916\n"
     ]
    }
   ],
   "source": [
    "#Step4，模型训练，得出预测结果\n",
    "\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train,X_valid,Y_train,Y_valid=train_test_split(train.drop('Attrition',axis=1),train['Attrition'],test_size=0.2,random_state=42)\n",
    "tpot=TPOTClassifier(generations=5,population_size=20,verbosity=2)\n",
    "tpot.fit(X_train,Y_train)\n",
    "print(tpot.score(X_valid,Y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=tpot.predict_proba(test)[:, 1]\n",
    "test['Attrition']=predict\n",
    "test[['user_id','Attrition']].to_csv('Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('tpot_pipeline.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mTPOTClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpopulation_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffspring_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutation_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrossover_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_time_mins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_eval_time_mins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_dask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiodic_checkpoint_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable_update_check\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      TPOT estimator for classification problems.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Set up the genetic programming algorithm for pipeline optimization.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "generations: int or None, optional (default: 100)\n",
       "    Number of iterations to the run pipeline optimization process.\n",
       "    It must be a positive number or None. If None, the parameter\n",
       "    max_time_mins must be defined as the runtime limit.\n",
       "    Generally, TPOT will work better when you give it more generations (and\n",
       "    therefore time) to optimize the pipeline. TPOT will evaluate\n",
       "    POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
       "population_size: int, optional (default: 100)\n",
       "    Number of individuals to retain in the GP population every generation.\n",
       "    Generally, TPOT will work better when you give it more individuals\n",
       "    (and therefore time) to optimize the pipeline. TPOT will evaluate\n",
       "    POPULATION_SIZE + GENERATIONS x OFFSPRING_SIZE pipelines in total.\n",
       "offspring_size: int, optional (default: None)\n",
       "    Number of offspring to produce in each GP generation.\n",
       "    By default, offspring_size = population_size.\n",
       "mutation_rate: float, optional (default: 0.9)\n",
       "    Mutation rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
       "    This parameter tells the GP algorithm how many pipelines to apply random\n",
       "    changes to every generation. We recommend using the default parameter unless\n",
       "    you understand how the mutation rate affects GP algorithms.\n",
       "crossover_rate: float, optional (default: 0.1)\n",
       "    Crossover rate for the genetic programming algorithm in the range [0.0, 1.0].\n",
       "    This parameter tells the genetic programming algorithm how many pipelines to\n",
       "    \"breed\" every generation. We recommend using the default parameter unless you\n",
       "    understand how the mutation rate affects GP algorithms.\n",
       "scoring: string or callable, optional\n",
       "    Function used to evaluate the quality of a given pipeline for the\n",
       "    problem. By default, accuracy is used for classification problems and\n",
       "    mean squared error (MSE) for regression problems.\n",
       "\n",
       "    Offers the same options as sklearn.model_selection.cross_val_score as well as\n",
       "    a built-in score 'balanced_accuracy'. Classification metrics:\n",
       "\n",
       "    ['accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy',\n",
       "    'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted',\n",
       "    'precision', 'precision_macro', 'precision_micro', 'precision_samples',\n",
       "    'precision_weighted', 'recall', 'recall_macro', 'recall_micro',\n",
       "    'recall_samples', 'recall_weighted', 'roc_auc']\n",
       "\n",
       "    Regression metrics:\n",
       "\n",
       "    ['neg_median_absolute_error', 'neg_mean_absolute_error',\n",
       "    'neg_mean_squared_error', 'r2']\n",
       "cv: int or cross-validation generator, optional (default: 5)\n",
       "    If CV is a number, then it is the number of folds to evaluate each\n",
       "    pipeline over in k-fold cross-validation during the TPOT optimization\n",
       "     process. If it is an object then it is an object to be used as a\n",
       "     cross-validation generator.\n",
       "subsample: float, optional (default: 1.0)\n",
       "    Subsample ratio of the training instance. Setting it to 0.5 means that TPOT\n",
       "    randomly collects half of training samples for pipeline optimization process.\n",
       "n_jobs: int, optional (default: 1)\n",
       "    Number of CPUs for evaluating pipelines in parallel during the TPOT\n",
       "    optimization process. Assigning this to -1 will use as many cores as available\n",
       "    on the computer. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used.\n",
       "    Thus for n_jobs = -2, all CPUs but one are used.\n",
       "max_time_mins: int, optional (default: None)\n",
       "    How many minutes TPOT has to optimize the pipeline.\n",
       "    If not None, this setting will allow TPOT to run until max_time_mins minutes\n",
       "    elapsed and then stop. TPOT will stop earlier if generationsis set and all\n",
       "    generations are already evaluated.\n",
       "max_eval_time_mins: float, optional (default: 5)\n",
       "    How many minutes TPOT has to optimize a single pipeline.\n",
       "    Setting this parameter to higher values will allow TPOT to explore more\n",
       "    complex pipelines, but will also allow TPOT to run longer.\n",
       "random_state: int, optional (default: None)\n",
       "    Random number generator seed for TPOT. Use this parameter to make sure\n",
       "    that TPOT will give you the same results each time you run it against the\n",
       "    same data set with that seed.\n",
       "config_dict: a Python dictionary or string, optional (default: None)\n",
       "    Python dictionary:\n",
       "        A dictionary customizing the operators and parameters that\n",
       "        TPOT uses in the optimization process.\n",
       "        For examples, see config_regressor.py and config_classifier.py\n",
       "    Path for configuration file:\n",
       "        A path to a configuration file for customizing the operators and parameters that\n",
       "        TPOT uses in the optimization process.\n",
       "        For examples, see config_regressor.py and config_classifier.py\n",
       "    String 'TPOT light':\n",
       "        TPOT uses a light version of operator configuration dictionary instead of\n",
       "        the default one.\n",
       "    String 'TPOT MDR':\n",
       "        TPOT uses a list of TPOT-MDR operator configuration dictionary instead of\n",
       "        the default one.\n",
       "    String 'TPOT sparse':\n",
       "        TPOT uses a configuration dictionary with a one-hot-encoder and the\n",
       "        operators normally included in TPOT that also support sparse matrices.\n",
       "template: string (default: None)\n",
       "    Template of predefined pipeline structure. The option is for specifying a desired structure\n",
       "    for the machine learning pipeline evaluated in TPOT. So far this option only supports\n",
       "    linear pipeline structure. Each step in the pipeline should be a main class of operators\n",
       "    (Selector, Transformer, Classifier or Regressor) or a specific operator\n",
       "    (e.g. SelectPercentile) defined in TPOT operator configuration. If one step is a main class,\n",
       "    TPOT will randomly assign all subclass operators (subclasses of SelectorMixin,\n",
       "    TransformerMixin, ClassifierMixin or RegressorMixin in scikit-learn) to that step.\n",
       "    Steps in the template are delimited by \"-\", e.g. \"SelectPercentile-Transformer-Classifier\".\n",
       "    By default value of template is None, TPOT generates tree-based pipeline randomly.\n",
       "warm_start: bool, optional (default: False)\n",
       "    Flag indicating whether the TPOT instance will reuse the population from\n",
       "    previous calls to fit().\n",
       "memory: a Memory object or string, optional (default: None)\n",
       "    If supplied, pipeline will cache each transformer after calling fit. This feature\n",
       "    is used to avoid computing the fit transformers within a pipeline if the parameters\n",
       "    and input data are identical with another fitted pipeline during optimization process.\n",
       "    String 'auto':\n",
       "        TPOT uses memory caching with a temporary directory and cleans it up upon shutdown.\n",
       "    String path of a caching directory\n",
       "        TPOT uses memory caching with the provided directory and TPOT does NOT clean\n",
       "        the caching directory up upon shutdown. If the directory does not exist, TPOT will\n",
       "        create it.\n",
       "    Memory object:\n",
       "        TPOT uses the instance of joblib.Memory for memory caching,\n",
       "        and TPOT does NOT clean the caching directory up upon shutdown.\n",
       "    None:\n",
       "        TPOT does not use memory caching.\n",
       "use_dask: boolean, default False\n",
       "    Whether to use Dask-ML's pipeline optimiziations. This avoid re-fitting\n",
       "    the same estimator on the same split of data multiple times. It\n",
       "    will also provide more detailed diagnostics when using Dask's\n",
       "    distributed scheduler.\n",
       "\n",
       "    See `avoid repeated work <https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html#avoid-repeated-work>`__\n",
       "    for more details.\n",
       "periodic_checkpoint_folder: path string, optional (default: None)\n",
       "    If supplied, a folder in which tpot will periodically save pipelines in pareto front so far while optimizing.\n",
       "    Currently once per generation but not more often than once per 30 seconds.\n",
       "    Useful in multiple cases:\n",
       "        Sudden death before tpot could save optimized pipeline\n",
       "        Track its progress\n",
       "        Grab pipelines while it's still optimizing\n",
       "early_stop: int or None (default: None)\n",
       "    How many generations TPOT checks whether there is no improvement in optimization process.\n",
       "    End optimization process if there is no improvement in the set number of generations.\n",
       "verbosity: int, optional (default: 0)\n",
       "    How much information TPOT communicates while it's running.\n",
       "    0 = none, 1 = minimal, 2 = high, 3 = all.\n",
       "    A setting of 2 or higher will add a progress bar during the optimization procedure.\n",
       "disable_update_check: bool, optional (default: False)\n",
       "    Flag indicating whether the TPOT version checker should be disabled.\n",
       "\n",
       "\n",
       "Returns\n",
       "-------\n",
       "None\n",
       "\u001b[1;31mFile:\u001b[0m           d:\\anocconda3-5.20\\lib\\site-packages\\tpot\\tpot.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TPOTClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
